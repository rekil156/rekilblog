[
  {
    "objectID": "posts/lesson9_stableDissufion/Lesson9.html",
    "href": "posts/lesson9_stableDissufion/Lesson9.html",
    "title": "A different way to look at Stable Diffusion",
    "section": "",
    "text": "These are the notes from Lesson 9 of fastai part 2, which is not yet public (may be public by Dec 2022)\nThe way stable diffusion is normally explained is focused heavily on a particular mathematical derivation. We’ve been developing a totally new way of thinking about stable diffusion and that is what we’ll be seeing. It’s mathematically equivalent to the approach you’ll see in other places but what you’ll realize is that this is actually conceptually much simpler and also later in this course we’ll be showing you some really innovative directions that this can take you when you think of it in this brand new way. I’m expressing it in a different way and it’s equally mathematically valid."
  },
  {
    "objectID": "posts/lesson9_stableDissufion/Lesson9.html#the-magic-function",
    "href": "posts/lesson9_stableDissufion/Lesson9.html#the-magic-function",
    "title": "A different way to look at Stable Diffusion",
    "section": "The magic function",
    "text": "The magic function\nLet’s imagine that we are trying to get something to generate handwritten digits ie stable diffusion for handwritten digits. How do we go about it?\nWe’re going to start by assuming there’s some function or API(black box for now,f- magic function), that takes in a handwritten digit and spits out the probability of it being a handwritten digit. For example we pass in an image X1 and it spits back p(X1) = 0.98 ie probability that X1 is a handwritten digit, X1 corresponds to the digit 3 in the figure below\n\nWhy is this magic function interesting ? We can use this magic function to actually generate handwritten digits.\nImage X3 in the figure doesn’t look like a digit. Let’s see how we could try to convert it into a handwritten digit. It is a 28x28 image with 784 pixels.\n\nSo let’s slightly alter each of the pixels, and each time we alter a pixel we pass it to the magic function and see how the probability changes. We want to make changes to the image with the hope that the probability value of it being a handwritten digit increases.\n\nLet’s look at a specific example,image X3. Handwritten digits don’t normally have any pixels that are black in the very bottom edge(red box), so if we made it a little bit lighter and passed it through our magic function the probability would probably go up a tiny bit(say 0.02 to 0.023).\nSo we could do that for every single pixel of the 28x28 image one at a time. We want to find out which pixels we should be making a little bit lighter and which pixels we should be making a little bit darker to make the image look more like a handwritten digit.\nPutting this mathematically - we want the gradient of the probability that X3 is a handwritten digit with respect to the pixels of X3\n\nNote: We didn’t say \\(∂ p(X3)/ ∂ X3\\) which you might be familiar with and the reason for that is that we’ve calculated this \\(∂ p(X3)/ ∂ X3\\) for every single pixel and so when you do it for lots of different inputs you have to turn the ∂ into a ∇ del or a nabla.\nSo this means that this \\(∇p(X3)/ ∇ X3\\) has 784 values (28x28 image). They tell us how we can change X3 to make it look more like a digit. We can now change the pixels according to this gradient and this is a lot like what we do when we train neural networks. Except instead of changing the weights in a model we’re changing the inputs to the model i.e. the image pixels. So we’re going to take every pixel and subtract a little bit of its gradient (c * gradient, where c can be thought of as a learning rate) to get a new image,X3′, which looks slightly more like a handwritten digit than before.\n\nSo now we can pass this new updated image(X3′) through our magic function to calculate a new score and repeat this process.\nSo if we have this magic function we can use it to turn any noisy input into something that looks like a handwritten digit(something with a high probability score). Key thing to remember: as I change the input pixels I get back a probability score that tells me if this image is a handwritten digit.\nSo if we do this by changing each pixel one at a time to calculate a derivative i.e. finite differencing method of calculating derivatives is very slow. Luckily we can use f.backward() and then X3.grad will have the same thing but all in one go by using the analytic derivatives. So now if we have f.backward() and X3.grad we really don’t need the magic function, f. \nWe can now multiply the gradient by a small constant c and subtract it from the pixels, we’ll do it a few times so that we get larger and larger probabilities of this being a handwritten digit.\nSo this magic function will be our neural network which we train to tell us which pixels to change to make an image look more like a handwritten digit.\nSo next we need some training data. We create this data by using real handwritten digits and then just chucking random noise on top of it. it’s a little bit awkward for us to come up with an exact score which can tell us how much these noisy images are like a handwritten digit so instead let’s predict how much noise was added. This slightly noisy number seven(in the figure below) is actually equal to the original number seven plus some noise.\n\nSo we generate this data and then rather than trying to come up with the arbitrary probability of predicting how much of a handwritten digit it is we say the amount of noise tells us how much like a digit it is. So something with no noise is very much like a digit(digit 9 in the figure above) and something with lots of noise isn’t much like a digit at all(digit 6 in the figure above) .\n\nSo let’s create a neural net for which we need:\n\nInputs -noisy digits\n\nOutputs - noise\n\nloss function - MSE, between the predicted output(noise) and the actual noise\n\nWith this we have the ability to know how much do we have to change a pixel by to make it look more like a digit, this is exactly what we wanted - \\(∇p(X3)/ ∇ X3\\) So once we train the neural net we can pass it an image (random noise) and it’s going to spit out information saying which part of that image it think’s is noise and it’s going to leave behind the bits that look the most like a digit but it won’t do this in one step. It will do this iteratively, we’ll see why later.\n\nSo we can repeat this again and again and you can see now why we are doing this multiple times"
  },
  {
    "objectID": "posts/lesson9_stableDissufion/Lesson9.html#building-blocks-of-stable-diffusion",
    "href": "posts/lesson9_stableDissufion/Lesson9.html#building-blocks-of-stable-diffusion",
    "title": "A different way to look at Stable Diffusion",
    "section": "Building blocks of stable diffusion",
    "text": "Building blocks of stable diffusion\nSo now with this groundwork laid lets see the building blocks of stable diffusion\n\n1) UNET\n\nLet’s look at the input and output of the Unet.\n\nInput - somewhat noisy image, it could be no noisy at all or it could be all noise\n\nOutput - noise\nSo if we subtract the output from the input we end up with an approximation of the unnoisy image\n\nSo our handwritten input image is 28x28, but in reality we would want to generate bigger images. Currently, these models work with 512x512x3 images. So for training this model we use millions of noisy versions of these 512x512x3 images. It is going to take a long time to train it. How can we make this faster? Do we really need to store each and every pixel value? We know pixel values don’t change much locally. Can we use this insight? For example a JPEG picture is far fewer bytes than the number of bytes you would get if you multiplied its height by its width by its channels. So the idea is to use compression.\n\n\n2)VAE - Variational autoencoders\nSo let’s see how to compress it with an autoencoder(AE).\nArchitecture of an autoencoder\nAt each level we will double the number of channels and use a stride two convolution and at the end we add a few resnet like blocks to squish down the number of channels from 24 to 4.\n\nSo we started with a 512x512x3 image and we have a representation of this image which has a size of 64x64x4, we have compressed it by a factor of 48. This representation is called latents. What we just saw is an encoder, We are encoding the “big” image to a much “smaller” representation. This factor of compression makes sense depending on how well we can reconstruct the original image back from these latents of size 64x64x4. So let’s build the inverse process to decode these latents, decoder. And then we can put the encoder and decoder together and train it.\n\nWe can use MSE and train this, in the beginning we will get random outputs but later we should get close to our input\n\nSo what is the point of a model that spits back an output that is identical to the input?\n\nThe green bit(when we go from a larger image to a smaller representation) is the encoder and the red bit is the decoder. Say I want to send an image to someone, I could pass it through the encoder and I’ve now got something that’s 48 times smaller than my original picture. The person who receives this can pass it through the decoder(he has a copy of the trained decoder) to get back the original image. This is basically a compression algorithm.\nSo how can we use this compression algorithm?\nWe can pass these “smaller” latents as the input to the Unet instead of passing the “bigger” original images.\nSo let us update the inputs and outputs of the Unet:\n\nInput - somewhat noisy latents\n\nOutput - noise\n\nNow we can subtract the output from the input to get the denoised latents and pass it to the decoder of theautoencoder to get the best approximation of the denoised version of the image.\nThis autoencoder in practice is a Variational Autoencoder.\nSo let’s recap what we have done so far. We started with a 512x512x3 image, passed it through the VAE’s encoder to get a compressed version of size 64x64x4. These latents are then passed through the unet which predicts the noise. We can subtract this noise from the encoder’s latents to get denoised latents. These denosied latents are passed through the decoder of the VAE to generate an image of size 512x512x3.\nFew points to keep in mind:\n\nThe VAE is an optional building block. It has the advantage of training the Unet with smaller size latents rather than images, so it’s faster.\nThe encoder of the VAE is only required during training and not during inference.\n\n\n\n3)CLIP\nSo next let’s see how the text prompts play a role. Rather than just feeding in noise and getting back some digit, can we ask it to generate a specific number, say “3”.\n\nTo achieve this, in addition to passing in the noisy input image, let’s also pass in a one hot encoded version of the digit “3”.\n\nSo we’re now passing two things into this model, the image pixels and what digit it is in one hot encoded vector form. So the model is going to learn how to predict what the noise is and since it has this extra information of what the original digit was, we can expect this model to be better at predicting noise than the previous one.\nAfter the model has been trained, if we feed in “3”(one hot encoded vector) and the noise, it is going to say the noise is everything that doesn’t represent the number three. So this is called guidance. We can use that guidance to guide the model as to what image we want it to create.\nIs one hot encoded vector the best way though? Say we want to create an image from the phrase - “a cute teddy”. If we were to use 1- hot encoded vectors then we have to create a 1-hot encoded vector for every phrase, which seems very inefficient.\nWe’ll create a model that can use the phrase -”a cute teddy” as an input and can output a vector of numbers,embeddings, that in some way represents what “cute teddies” look like.\nSo we can gather images from the internet and if they have alt tags they will have some description of the image i.e. a text associated with that image.\n\nNow we can create two models, one model which is a text encoder and one model which is an image encoder.\n\nSo we can pass the image to the image encoder and text to the text encoder and they will each give us two embeddings.\nNow when we pass the image of the swan, through our image model we would like it to return embeddings which are similar to the embeddings that we get when we pass the text “the graceful swan” through the text encoder. In other words we want their embeddings to be similar. How do we tell our model to do this? We can use dot product to check for similarity between the embeddings. Higher the dot product more similar are the embeddings.\n\nSo now we have a grid of images and text, each combination of their embeddings will give us a score when we take their dot product. We want the dot product for only the matching image-text pairs to be high(blue,diagonal element) and similarly we want the non matching pairs of text and image to be small(red, off diagonal elements)\n\nSo our loss function can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.\n\nIf we want this loss function to be good then we’re going to need the weights of our model for the text encoder to spit out embeddings that are very similar to the image embeddings that they’re paired with and not similar to the embedding of the images they are not paired with.\nNow we can feed our text encoder with “a graceful swan”, “some beautiful swan”, “such a lovely swan” and these should all give very similar embeddings because these would all represent very similar images of swans.\nWe’ve successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model.\nSo we took this detour because creating 1-hot encoded vectors for all the possible phrases was impractical. We can can now take our phrase - “a cute teddy bear” and feed it in text encoder to get out some features/embeddings.\n\nThese features are what we will use as guides instead of the 1- hot encoded vectors when we train our Unet. So we pass the phrase - “a cute teddy’ to our text encoder, which will generate embeddings which is going to be used as a guide by our model to turn the input noisy image into something that is similar to things that it’s previously seen that are “cute teddies”\nThis pair of models have a name - CLIP,Contrastive Language-Image Pre-training and the loss we are using is called contrastive loss.\nLet us see what all building blocks we have so far.\n\n\nwe’ve got a Unet that can denoise latents into unnoisy latents\nwe’ve got the decoder of VAE that can take latents and create an image\nwe’ve got the CLIP text encoder which can guide the Unet with captions\n\nStable diffusion is a latent diffusion model and what that means is that it doesn’t operate in the pixel space, it operates on in the latent space of some other autoencoder model and in this case that is a variational autoencoder.\nSome jargon:\n\nscore function\ntime-steps.\n\n\nThese gradients that we have are often called the score function.\n“Time steps” is the jargon used in a lot of papers but we never used any time steps during our training. This is basically an overhang from how the math was formulated in the initial papers. We will avoid using the term time steps but we can see what time steps are though it’s got nothing to do with time in real life.\nWe added varying levels of noise to our images, some were very noisy, some were not noisy at all, some had no noise and some would have been pure noise.\nLet’s create a noising schedule,which is a monotonically decreasing function. Where say the x-axis(“t”) varies from one to a thousand. Now we randomly pick a number between one and thousand, we look up in the noise schedule and return the sigma(or beta is what you’ll see in papers). Say we happen to pick the number four, we would look up to find a value on the y-axis which is the sigma, the amount of noise to add to your image if you randomly picked a four.\n\nIf you randomly pick one you’re going to have a lot of noise and if you pick a 1000 you’re going to have hardly any noise.\nSo when we are training we need to pick some random amount of noise for every image. One way to get the random noise is to pick a random number between one to a thousand, look it up on this noise scheduler function and that will tell us the sigma of the noise to be added.\nPeople refer to this t as the time step, nowadays you don’t really have to look up in the noise scheduler. A lot of people are starting to get rid of this idea altogether and some people instead will simply say how much noise was there.\nSo each time you create a mini batch during training, you randomly pick a batch of images from your training set, you randomly pick either an amount of noise or you randomly pick a t and then look up the amount of noise and then use that amount of noise to create the noisy images. Then you pass that mini batch into your model to train it and that trains the weights in your model so it can learn to predict noise.\n\nHow exactly do we do this inference process ? When you’re generating a picture from pure noise, this corresponds to t=0 on the noise scheduler where you have maximum noise. So you want it to learn to remove noise but if you do this in one step you’ll end up with bad images.\n\nSo in practice we get the prediction of the noise and then we multiply it by some constant,c, which is like a learning rate but here we’re not updating weights now we’re updating pixels and we subtract it from the original noisy pixels. So it doesn’t actually predict the final denoised image, all it does is remove some small factor of the noise to give us a slightly denoised image.\nThe reason we don’t jump all the way to the final image is because things that look like the image we got by using t=1 (crappy image) never appeared in our training set(does this mean we never train with highly noised images??) and so since it never appeared in our training set our model has no idea what to do with it. Our model only knows how to deal with things that look like somewhat noisy latents and so that’s why we subtract just a small factor of the noise so that we still have a somewhat noisy latent for this process to repeat a bunch of times.\nThe questions of what c to use and how do we go from the prediction of noise to what we subtract are the kind of the things that you decide in the actual diffusion sampler. The sampler is used to both add the noise and how to subtract the noise\nThis looks a lot like deep learning optimizers. If you change the same parameters by a similar amount multiple times in multiple steps maybe you should increase the amount you change them, this concept is something we call Momentum. We even have better ways of doing that where we say well what happens as the variance changes. Maybe we can look at that as well and that gives us something called Adam and these are types of optimizers. All diffusion-based models came from a very different world of math, which is the world of differential equations. There are a lot of parallel concepts in these two worlds. Differential equations is all about how to take bigger steps instead of taking smaller steps so we can converge quicker. Differential equation solvers use a lot of the same kind of ideas as optimizers, if you squint. One thing that differential equations solvers do which is that they tend to take t as an input and in fact pretty much all diffusion models do that too, we hadn’t spoken about that.\n\nPretty much all diffusion models don’t just take the input pixels and the caption, they also take t. And the idea is that the model will be better at removing the noise if you tell it how much noise there is and remember t is related to how much noise there is.\nJeremy very strongly suspects that this premise is incorrect because for a fancy neural net figuring out how noisy something is very simple. So when you don’t need to pass in t things stop looking like differential equations and they start looking more like optimizers\nWe could swap MSE with perceptual loss. All these things suddenly become possible when we start thinking of this as an optimization problem rather than a differential equation solving problem"
  },
  {
    "objectID": "posts/diffEdit/DiffEdit_blog.html",
    "href": "posts/diffEdit/DiffEdit_blog.html",
    "title": "A peek into the DiffEdit paper",
    "section": "",
    "text": "We’ll be looking at the DiffEdit paper and implement it with Stable Diffusion.\nDiffEdit uses test-conditioned diffusion models to automatically generate semantic maps. This means that instead of generating the mask(semantic maps) manually we can generate them by only providing a text prompt of what we want to edit. Let’s look at an image from the paper to understand what this means.\nGiven the input image and query text - “A bowl of pears” we want to modify the input image to reflect this query while not modifying the rest of the image.\nWe can see from the output image, “Masked Diffusion”, the bowl, table, background wall are all intact and only the contents of the bowl have changed, reflecting our query - “A bowl of pears”.\nSo our input image got modified to the output image based on the query while keeping the rest of the image unchanged.\nWe have another image in the figure above - “Generate Mask”. Prior to this paper users would have to manually draw this mask(the region in red) and then feed it along with the input image to an inpainting algorithm. Now with diffEdit we can automatically generate the mask based on thequery."
  },
  {
    "objectID": "posts/diffEdit/DiffEdit_blog.html#setup-stable-diffusion-with-hugging-face",
    "href": "posts/diffEdit/DiffEdit_blog.html#setup-stable-diffusion-with-hugging-face",
    "title": "A peek into the DiffEdit paper",
    "section": "Setup Stable Diffusion with Hugging Face",
    "text": "Setup Stable Diffusion with Hugging Face\nWe will be using Stable Diffusion as our diffusion model.Hugging Face’s diffusers library makes this easily accesible. You’ll need to accept the license and get a token.\nA lot of this notebook is derived from the excellent fastai course taught by Jeremy Howard, Jonathan Whitaker, Tanishq Mathew Abraham, Wasim Lorgat\n\n\nCode\n!pip install -q --upgrade transformers diffusers ftfy\n\n\n\n\nCode\n# You need your token \nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\nCode\nimport torch\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers import logging\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\nfrom tqdm.auto import tqdm\nfrom torch import autocast\nimport PIL\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport numpy\nfrom torchvision import transforms as tfms\nfrom fastdownload import FastDownload\nimport numpy as np\n\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nLet’s set up the vae , text_encoder and unet\n vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n\nCode\n# Load the autoencoder model which will be used to decode the latents into image space. \nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Load the tokenizer and text encoder to tokenize and encode the text. \ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n# The noise scheduler\nscheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n\n# To the GPU we go!\nvae = vae.to(torch_device)\ntext_encoder = text_encoder.to(torch_device)\nunet = unet.to(torch_device);\n\n\n\nHelper functions to convert between the latent space and image space, tensors and pillow etc\n\n\nCode\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode((tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1)) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # batch of latents -> list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\ndef pil_to_tensor(input_im):\n        return tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)\n\ndef tensor_to_pil(image):\n    image = image.clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\n\n\nGetting the input image\nWe’ll be using this image of a horse, from Depositphotos,Creator: melory melory.\n\n\n\n\n\n\n\nSTEP 1 Compute Mask\n Given an input image and two different prompts - “Reference Text R”, promptR, and “Query Text Q”,promptQ, the text-conditioned diffusion model will generate different noise estimates noiseR and noiseQ respectively. The difference between these noise estimates helps generate the mask. With this mask we can can identify the regions in the image that need to be altered to match the Query Text Q.\nLet’s see how we can achieve this:\n(section 3.2 SEMANTIC IMAGE EDITING WITH DIFFEDIT in the paper)\n1. add Gaussian noise with strength 50%\n2. remove extreme values in noise predictions\n3. stabilize the effect by averaging spatial differences over a set of n input noises, where n = 10\n4. binarize with a threshold of 0.5\nLet us follow those general steps and create our generate_noise function.\nFirst let’s tokenize and encode our text to get the embeddings:\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n\n\nCode\ndef get_text_embeddings(prompt): \n    # get prompt text embeddings\n    text_inputs = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n      text_embeddings = text_encoder(text_inputs.input_ids.to(torch_device))[0]\n    \n    uncond_input = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n    return text_embeddings\n\n\nNext we will need to convert our image into latent space using the VAE’s encoder and then add noise to it:\nencoded = pil_to_latent(img)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\nWe pass the text_embeddings and the latents to the unet to predit an estimate of the noise:\nnoise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n\nCode\ndef generate_noise(img,prompt,start_step,seed):\n  guidance_scale = 8                  # Scale for classifier-free guidance\n  generator = torch.manual_seed(seed)\n  batch_size = 1\n\n  num_inference_steps=50\n  torch.manual_seed(seed)\n\n  # Prep text\n  text_embeddings = get_text_embeddings(prompt)\n\n  # Prep Scheduler (setting the number of inference steps)\n  scheduler.set_timesteps(num_inference_steps)#add back\n\n  #Prep the image\n  encoded = pil_to_latent(img)\n\n  #Add noise\n  torch.manual_seed(seed)\n  noise = torch.randn_like(encoded)\n  t = torch.tensor([scheduler.timesteps[start_step]]).cuda()\n  latents = scheduler.add_noise(encoded, noise, timesteps=t)\n  latents = latents.to(torch_device).float()\n\n  latent_model_input = torch.cat([latents] * 2)\n  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n  # predict the noise residual\n  with torch.no_grad():\n      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n  # perform guidance\n  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n  # compute the previous noisy sample x_t -> x_t-1\n  # latents = scheduler.step(noise_pred, t, latents).prev_sample\n  latents_x0= scheduler.step(noise_pred, t, latents).pred_original_sample\n  \n  # return latents_to_pil(latents_x0)[0]\n  # return latents_to_pil(noise_pred)[0]\n  return noise_pred\n\n\nNote: We are converting these noise predictions to images so we can visualize it.\nLet us use our generate_noise function and pass in the image, text prompt and see what we get:\npromptR = \"a horse in a field\"\npromptQ = \"a zebra in a field\"\n\n\nCode\nstart_step = 25 \npromptR = \"a horse in a field\"\npromptQ = \"a zebra in a field\"\nnoiseR = generate_noise(img, promptR, start_step=start_step, seed=1)\nnoiseQ = generate_noise(img, promptQ, start_step=start_step, seed=1)\nimage_grid([latents_to_pil(noiseR)[0],latents_to_pil(noiseQ)[0]],1,2)\n\n\n\n\n\nHmm, that doesn’t look very informative. But remember in Stable Diffusion we iteratively predict how much noise needs to be removed and remove a fraction of it, so that our generated image looks more like the input prompt. So lets take the absolute difference between the noises for each prompt.\n\n\nCode\ndef abs_difference(img1,img2):\n  img1, img2 = np.array(img1),np.array(img2)\n  abs = np.abs(img1*1. - img2*1.)\n  return Image.fromarray(np.uint8(abs))\n\n\n\n\nCode\nimage_grid([abs_difference(abs_difference(img,latents_to_pil(noiseR)[0]),abs_difference(img,latents_to_pil(noiseQ)[0]))],1,1)\n\n\n\n\n\nLet us calculate this absolute difference num_avg(n=10 in the paper) times, with different random noise. We could even augment the image for each iteration too.\nWe scale the image between [0,1]\nnoiseR = generate_noise(img,promptR,start_step=start_step,seed=n)\nnoiseQ = generate_noise(img,promptQ,start_step=start_step,seed=n)\n\nabs_diff = torch.abs(noiseR.cpu()-noiseQ.cpu())#absolute difference \nabs_diff_scaled = 255*((abs_diff - abs_diff.min())/(abs_diff.max()-abs_diff.min()))#scale\n\navg_lat = latents_to_pil(stack_of_diff.mean(0)[None].cuda())[0]\navg = np.array(avg_lat)\n\ndiff_bin = 255*((avg[:,:,0]>threshold) |(avg[:,:,1]>threshold) |(avg[:,:,2]>threshold))\n\n\nCode\n#noise latents\nnum_avg = 10\nstack_of_diff = torch.zeros((num_avg,4,size//8,size//8))\nfor n in tqdm(range(num_avg)): \n  noiseR = generate_noise(img,promptR,start_step=start_step,seed=n)\n  noiseQ = generate_noise(img,promptQ,start_step=start_step,seed=n)\n  abs_diff = torch.abs(noiseR.cpu()-noiseQ.cpu())#absolute difference \n  abs_diff_scaled = 255*((abs_diff - abs_diff.min())/(abs_diff.max()-abs_diff.min()))#scale\n  stack_of_diff[n] = abs_diff_scaled\n\n#noise latents averaaged\navg_lat = latents_to_pil(stack_of_diff.mean(0)[None].cuda())[0]\navg = np.array(avg_lat)\n\nthreshold = 210\ndiff_bin = 255*((avg[:,:,0]>threshold) |(avg[:,:,1]>threshold) |(avg[:,:,2]>threshold))\ndiff_bin = np.dstack((diff_bin,diff_bin,diff_bin))\nplt.imshow(np.uint8(diff_bin),cmap=\"gray\");\n\n\nNext we’ll binarize it with a threshold and do a few more operations like median filter, dilation, erosion etc to enhance the mask.\n\n\nCode\npil_im = Image.fromarray(np.uint8(diff_bin))\npil_im_ds = pil_im.resize((256,256),PIL.Image.NEAREST,)\n\npil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MedianFilter(size = 3)) \n# pil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MedianFilter(size = 3)) \n\npil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MaxFilter(3))\n# pil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MinFilter(3))\n# pil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MaxFilter(3))\n# pil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MaxFilter(3))\npil_im_ds = pil_im_ds.filter(PIL.ImageFilter.MinFilter(3))\n\n\nmask = pil_im_ds.resize((512,512),PIL.Image.NEAREST,)\nimage_grid([img,mask],1,2)\n\n\n\n\n\nFeathering\nThe mask has very sharp transitions at the edges, we can mitigate this by applying a gaussian filter.\n\n\n\n\n\nSo we started with an encoded version of the image and added noise to it. We predict how much noise needs to be removed from this encoded version to match the prompts, promptR and promptQ which gave us noiseR and noiseQ. The difference between noiseR and noiseQ gave us the mask.\n\n\nSTEP 2 Encode with DDIM until encoding ratio r\n All we do here is add noise to the input image iteratively, this is called the reverse diffusion process. This is done with the unconditional model, that means we don’t use any prompt for this step.\nWe encode our image and add noise to it, very similar to Step 1. There are two differnces from Step 1 though:\n\nWe don’t use a prompt\n\nIn Step 1 we added the noise only for one timestep but here we progressively add it a bunch of times\n\nencoded = pil_to_latent(img)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([reversed(scheduler.timesteps)[i]]))\n\n\nCode\nstart_step=25\nencoded = pil_to_latent(img)\nstep2_latents=[]\nnoised_img=[]#progressively noisy images for display\nnum_inference_steps = 50            # Number of denoising steps\n\nscheduler.set_timesteps(num_inference_steps)\n\nlatents = encoded\nlatents = latents.to(torch_device).float()\nstep2_latents.append(latents)\n\nnoise = torch.randn_like(encoded)\nrev_sched_timesteps = reversed(scheduler.timesteps)\n# Loop - look at scheduler.sigmas, scheduler.sigmas[0] has max noise and scheduler.sigmas[-1] has least noise, so we need to reverse \nfor i, t in tqdm(enumerate(rev_sched_timesteps)):\n  if i < start_step-1:#Encoding ratio, xr\n    #print(i,t.item(),rev_sched_timesteps[i],reversed(scheduler.timesteps)[i])\n      \n      latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([rev_sched_timesteps[i]]))\n      latents = latents.to(torch_device).float()\n      step2_latents.append(latents)\n      noised_img.append(latents_to_pil(latents)[0])\nimage_grid(noised_img[0:start_step:2],1,5)  \n\n\n\n\n\n\n\n\nWe only noise the images upto a certain timestep, that corresponds to xr. This is the starting latent for the decoding in Step 3, \\(y_r = x_r\\). We don’t want to end up starting from complete noise. How many timesteps we choose effects the final image. Larger the timestep the edit will match the text query more strongly which allows for more modifications of the background.\nNote: scheduler.sigmas starts with the highest sigma for noise and gradually reduces to zero, we are seeing the reverse in the images above. This is because during the forward diffusison process we start from a completely noisy image, highest sigma and gradually reduce the noise to zero to get the output image.\nEncoding is also called reverse diffusion and decoding is called forward diffusion process.\n\nHow the encoding ratio effects the mask generated\nHere we look at the masks generated for the. following encoding_ratios - [10,15,20,25,30,35,40,45]\n\n\n\n\n\n\n\n\n\n\n\nSTEP 3 Decoding with mask guidance.\n So we have generated the mask, we have the latents of the gradually noised images from Step 2,[x1, … xt, … xr]\nWe use our diffusion model to decode the step 2 latents it with our text query Q, e.g. zebra. We use the mask M to guide this diffusion process by replacing pixels outside the mask with the latents xt, this would keep these parts of the image similar to the input. Whereas within the mask, M, the diffusion model is guided with the text querry.\nThis update can be written as: \\[\\hat{y}_t = My_t + (1 − M )x_t\\]\nWe tokenize and encode our promptQ(“a zebra on the field”) to get our embeddings:\ntext_input = tokenizer(promptQ, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nCopy over our latents from Step 2 at timestep r, \\(y_r = x_r\\)\nlatents = step2_latents[encoding_r_step]\nAnd we start our diffusion loop to predict noise and gradually remove it.\nnoise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\nlatents = scheduler.step(noise_pred, t, latents).prev_sample\nThen before we start the next iteration of diffusion we need to update our latents based on the mask.\nlatents = mask_ten[0,0,:,:] * latents + (1.0 - mask_ten[0,0,:,:]) * step2_latents[encoding_r_step]\n\n\nCode\ndiffusion_img_list=[]\nnum_inference_steps = 50            # Number of denoising steps\nguidance_scale = 5                  # Scale for classifier-free guidance\nbatch_size = 1\n\n# Prep text (same as before)\ntext_embeddings = get_text_embeddings(promptQ)\n\n# Prep Scheduler (setting the number of inference steps)\nscheduler.set_timesteps(num_inference_steps)\n\nencoding_r_step = start_step-1#30-1\n# print(f\"encoding_r_step: {encoding_r_step}\")\nlatents = step2_latents[encoding_r_step]#noised latent, xr from step 2\nlatents = latents.to(torch_device).float()\n\nmask_ten = pil_to_tensor(mask.resize((64,64),PIL.Image.NEAREST,))\n\n# Loop\nfor i, t in tqdm(enumerate(scheduler.timesteps)):\n    if i > num_inference_steps - start_step: \n        # print(f'for denoising: {i}, {encoding_r_step}') \n\n        latent_model_input = torch.cat([latents] * 2)\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)#??scale\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n        encoding_r_step = encoding_r_step - 1\n        latents = mask_ten[0,0,:,:] * latents + (1.0 - mask_ten[0,0,:,:]) * step2_latents[encoding_r_step]\n        updated_img = latents_to_pil(latents)[0]\n        diffusion_img_list.append(updated_img)\n\n\n\n\n\n\n\nOn the left we see the original horse image and we were able to reasonably convert it to a zebra without drawing the mask manually."
  },
  {
    "objectID": "posts/diffEdit/DiffEdit_blog.html#quick-note-on-inpaintng-vs-diffedit",
    "href": "posts/diffEdit/DiffEdit_blog.html#quick-note-on-inpaintng-vs-diffedit",
    "title": "A peek into the DiffEdit paper",
    "section": "Quick note on inpaintng vs diffEdit",
    "text": "Quick note on inpaintng vs diffEdit\nThe paper points out another drawback of inpainting - “(i) inpainting discards information about the input image that should be used in image editing (e.g. changing a dog into a cat should not modify the animal’s color and pose)” let’s see if we can compare them and see what are the effects of this.\nThe main contribution of the diffEdit paper is the automatic mask generation. Once this mask is generated we could use it for the inpainting algorithm. Let’s see what the difference in the results are and try to understand what is happening.\n\n\nCode\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom PIL import ImageDraw\n\npipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n\n\n\nCode\nmsk = mask.resize((512,512),PIL.Image.NEAREST,)\ntorch.manual_seed(100)\nfrom torch import autocast\nwith autocast(\"cuda\"):\n    images = pipe_inpaint(prompt=\"a zebra on the beach \", init_image=img, mask_image=msk,strength=0.8,guidance_scale=7.5, num_inference_steps=50).images\nimage_grid([img,images[0]],1,2)\n\n\n\n\n\nThe stripes in the inpainting case looks much better.\nWhy is this?\nWith the diffEdit algorithm we are uaing the mask and mixing in the latent space so we see the zebra still retains the black color of the horse (the input latents from Step 2 are being mixed in).\nIn the latent space the the parts that are masked are not lost completely since the neighboring pixels in the latent space have information about it (due to compression).\nIn the painting case we do not mix the latents. The part that is masked out is never seen by the model, it is discarded. Hence it generates cleaner white and black stripes.\nSo is inpainting better than diffEdit?\nWell in this case the inpainting seems to produce the cleaner black and white stripes.\nBut as the paper points out “(i) inpainting discards information about the input image that should be used in image editing (e.g. changing a dog into a cat should not modify the animal’s color and pose)” so instances when we want color and pose to be retained diffEdit should perform much better than inpainting.\n\nThings to try\n\nStart with a smaller input image size instead of 512x512, since we are generating a rough mask, we can process it much faster.\nWhile genrating the mask over n iterations with different noise, we can use augmentaion(change the brightness, sharpness etc)\nFor Step 1, go through the whole diffusion process and accumulate the noise and use that for mask generation."
  },
  {
    "objectID": "posts/diffEdit/DiffEdit_blog.html#credit",
    "href": "posts/diffEdit/DiffEdit_blog.html#credit",
    "title": "A peek into the DiffEdit paper",
    "section": "Credit",
    "text": "Credit\nThis notebook derives heavily from the Stable Diffusion Deep Dive Notebook from Jonathan Whitaker"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rekilblog",
    "section": "",
    "text": "Fastai 2022 part 2\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nRekil Prashanth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastai 2022 part 2\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nRekil Prashanth\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]